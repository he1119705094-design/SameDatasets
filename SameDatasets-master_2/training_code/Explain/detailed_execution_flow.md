# è¯¦ç»†æ‰§è¡Œæµç¨‹åˆ†æ

## ğŸš€ æ‚¨çš„å‘½ä»¤æ‰§è¡Œæµç¨‹

```bash
python train.py --name traditional_test --dataroot ../datasets --arch res50nodown --batch_size 8 --lr 0.0001
```

## ğŸ“‹ é€æ­¥æ‰§è¡Œåˆ†æ

### 1. å¯åŠ¨é˜¶æ®µ
```
train.py å¯åŠ¨
    â†“
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # è®¾ç½®ä½¿ç”¨GPU 0
    â†“
è§£æå‘½ä»¤è¡Œå‚æ•°
```

### 2. å‚æ•°è§£æç»“æœ
```python
opt.name = "traditional_test"
opt.dataroot = "../datasets"
opt.arch = "res50nodown"
opt.batch_size = 8
opt.lr = 0.0001
opt.patch_size = 16  # é»˜è®¤å€¼
opt.use_patch_model = False  # é»˜è®¤å€¼
opt.model_size = "small"  # é»˜è®¤å€¼
```

### 3. ç¯å¢ƒè®¾ç½®
```python
torch.manual_seed(opt.seed)  # è®¾ç½®éšæœºç§å­
```

### 4. æ•°æ®åŠ è½½
```
create_dataloader(opt, subdir="train", is_train=True)
    â†“
dataroot = "../datasets/train"
    â†“
get_dataset(opt, dataroot)  # å› ä¸ºæ²¡æŒ‡å®šbatched_syncing
    â†“
make_processing(opt)  # åˆ›å»ºæ•°æ®å˜æ¢
```

### 5. æ•°æ®å¤„ç†æµç¨‹
```
åŸå§‹å›¾åƒ
    â†“
make_pre(opt)  # é¢„å¤„ç†ï¼šç¼©æ”¾
    â†“
make_aug(opt)  # æ•°æ®å¢å¼ºï¼šæ—‹è½¬ã€ç¿»è½¬ç­‰
    â†“
make_post(opt)  # åå¤„ç†ï¼šè£å‰ªã€è°ƒæ•´å°ºå¯¸
    â†“
make_normalize(opt)  # å½’ä¸€åŒ–ï¼šToTensor + Normalize
    â†“
ImagePatchShuffle(patch_size=16)  # å›¾åƒå—æ‰“ä¹±
    â†“
æœ€ç»ˆtensor (B, C, H, W)
```

### 6. æ¨¡å‹åˆ›å»º
```
opt.use_patch_model = False
    â†“
æ‰§è¡Œ: print("ä½¿ç”¨ä¼ ç»ŸResNetè®­ç»ƒæ¨¡å‹")
    â†“
model = TrainingModel(opt, subdir="traditional_test")
```

### 7. TrainingModelåˆå§‹åŒ–
```python
self.device = torch.device('cuda:0')  # ä½¿ç”¨GPU
self.model = create_architecture("res50nodown", ...)  # åˆ›å»ºResNet-50
self.loss_fn = torch.nn.BCEWithLogitsLoss()  # äºŒåˆ†ç±»æŸå¤±
self.optimizer = torch.optim.Adam(...)  # Adamä¼˜åŒ–å™¨
```

### 8. è®­ç»ƒå¾ªç¯
```
for epoch in range(start_epoch, 1001):  # é»˜è®¤1000ä¸ªepoch
    â†“
    if epoch > start_epoch:
        # è®­ç»ƒé˜¶æ®µ
        for data in train_data_loader:
            loss = model.train_on_batch(data)
            # æ˜¾ç¤ºè®­ç»ƒæŸå¤±
        # ä¿å­˜æ¨¡å‹
    â†“
    # éªŒè¯é˜¶æ®µ
    y_true, y_pred, y_path = model.predict(valid_data_loader)
    acc = balanced_accuracy_score(y_true, y_pred > 0.0)
    # è®°å½•éªŒè¯å‡†ç¡®ç‡
    â†“
    # æ—©åœæ£€æŸ¥
    if early_stopping(acc):
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if early_stopping.early_stop:
            # é™ä½å­¦ä¹ ç‡æˆ–åœæ­¢è®­ç»ƒ
```

### 9. train_on_batchè¯¦ç»†æµç¨‹
```python
def train_on_batch(self, data):
    self.total_steps += 1
    self.model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    
    # æ•°æ®å‡†å¤‡
    input = data['img'].to(self.device)  # ç§»åŠ¨åˆ°GPU
    label = data['target'].to(self.device).float()
    
    # å‰å‘ä¼ æ’­
    output, feats = self.model(input, return_feats=False)
    
    # æŸå¤±è®¡ç®—
    loss = self.loss_fn(output.squeeze(1), label)
    
    # åå‘ä¼ æ’­
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()
    
    return loss.cpu()
```

### 10. éªŒè¯æµç¨‹
```python
def predict(self, data_loader):
    model = self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    with torch.no_grad():
        for data in data_loader:
            img = data['img']
            label = data['target'].cpu().numpy()
            
            # å‰å‘ä¼ æ’­
            out_tens, _ = model(img.to(self.device))
            out_tens = out_tens.cpu().numpy()[:, -1]
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            y_pred.extend(out_tens.tolist())
            y_true.extend(label.tolist())
    
    return y_true, y_pred, y_path
```

## ğŸ” å…³é”®æ‰§è¡Œç‚¹

### å›¾åƒå—æ‰“ä¹±çš„åº”ç”¨
- åœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼Œæ¯ä¸ªå›¾åƒéƒ½ä¼šç»è¿‡ `ImagePatchShuffle(patch_size=16)`
- å›¾åƒè¢«åˆ†å‰²æˆ16Ã—16çš„å°å—ï¼Œéšæœºæ‰“ä¹±åé‡æ–°ç»„åˆ
- è¿™ç ´åäº†å›¾åƒçš„å…¨å±€è¯­ä¹‰ï¼Œè¿«ä½¿æ¨¡å‹å…³æ³¨å±€éƒ¨ç‰¹å¾

### ResNetæ¨¡å‹å¤„ç†
- ä½¿ç”¨ `res50nodown` æ¶æ„ï¼ˆResNet-50 without downsamplingï¼‰
- æ¨¡å‹è¾“å‡ºå½¢çŠ¶: `(batch_size, 1)`
- ä½¿ç”¨ `BCEWithLogitsLoss` è¿›è¡ŒäºŒåˆ†ç±»

### è®¾å¤‡ä½¿ç”¨
- é»˜è®¤ä½¿ç”¨GPU 0è¿›è¡Œè®­ç»ƒ
- æ‰€æœ‰tensoréƒ½ä¼šç§»åŠ¨åˆ°GPU: `.to(self.device)`

## ğŸ“Š è¾“å‡ºä¿¡æ¯è§£è¯»

å½“æ‚¨è¿è¡Œå‘½ä»¤æ—¶ï¼Œä¼šçœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è¾“å‡ºï¼š
```
normalize RESNET
æ·»åŠ å›¾åƒå—æ‰“ä¹±å˜æ¢ï¼Œpatch_size=16
CLASSES: ['latent_diffusion_noise2image_churches', 'latent_diffusion_noise2image_FFHQ', 'latent_diffusion_text2img_set2']
#images     20 in ../datasets\valid/latent_diffusion_noise2image_churches
#images     22 in ../datasets\valid/latent_diffusion_noise2image_FFHQ
normalize RESNET
æ·»åŠ å›¾åƒå—æ‰“ä¹±å˜æ¢ï¼Œpatch_size=16
CLASSES: ['.']
#images    400 in ../datasets\train/.
RandomSampler: # [1 1]

# validation batches = 6
#   training batches = 50
ä½¿ç”¨ä¼ ç»ŸResNetè®­ç»ƒæ¨¡å‹
Arch: res50nodown with #trainable 23511397
lr: 0.0001
Validation ...
After 0 epoches: val acc = 0.475
Train loss: 0.6903: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:06<00:00,  1.50it/s] 
Validation ...
After 1 epoches: val acc = 0.3704545454545455
```

è¿™è¡¨ç¤ºï¼š
- ä½¿ç”¨äº†ResNetæ¨¡å‹ï¼ˆä¸æ˜¯CombinedDetectorï¼‰
- æ·»åŠ äº†å›¾åƒå—æ‰“ä¹±å˜æ¢
- è®­ç»ƒé›†æœ‰400å¼ å›¾åƒï¼ŒéªŒè¯é›†æœ‰42å¼ å›¾åƒ
- æ¨¡å‹æœ‰çº¦2350ä¸‡ä¸ªå¯è®­ç»ƒå‚æ•°
- å­¦ä¹ ç‡ä¸º0.0001
- ç¬¬ä¸€ä¸ªepochåéªŒè¯å‡†ç¡®ç‡ä¸º37%
